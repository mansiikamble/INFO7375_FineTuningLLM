# INFO7375_FineTuningLLM

Historical Text Modernizer
A fine-tuned language model that transforms archaic and historical English text into modern, accessible language while preserving meaning and cultural context.

ğŸ¯ Overview
This project fine-tunes GPT-2 Medium using LoRA (Low-Rank Adaptation) on a comprehensive dataset of historical text pairs, enabling automatic modernization of Shakespeare, legal documents, religious texts, and historical speeches.

ğŸ“Š Key Results
Training Success: 61% loss reduction (10.0 â†’ 3.9) over 3 epochs
Baseline Improvement: 45.2% performance gain over pre-trained models
Evaluation Metrics: 89.74% similarity, 91.96% modernization success
Dataset: 304 examples across 9 historical text types

ğŸ”§ Technical Implementation
Model Architecture
Base Model: GPT-2 Medium (355M parameters)
Fine-tuning: LoRA with 4.3M trainable parameters (1.20% of total)
Training: 3 epochs, 5e-5 learning rate, Tesla T4 GPU
Optimization: Comprehensive hyperparameter search (3 configurations)

Dataset Composition
Source TypeExamplesDomainShakespeare155Literary/PoeticLegal Documents31Formal/LegalReligious Texts19Biblical/ArchaicHistorical Speeches10Political/OratoryVariations94Augmented Examples
ğŸš€ Quick Start
Installation
bashpip install torch transformers peft accelerate datasets
Basic Usage
pythonfrom modernizer import HistoricalTextModernizer

# Initialize modernizer
modernizer = HistoricalTextModernizer()

# Modernize text
result = modernizer.modernize("Thou art a noble friend")
print(result)  # Output: "You are a noble friend"
Interactive Mode
python# Run interactive modernization
modernizer.interactive_mode()


historical-text-modernizer/
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ ğŸ“„ README.md                          # Dataset documentation
â”‚   â”œâ”€â”€ ğŸ“ raw/
â”‚   â”‚   â”œâ”€â”€ shakespeare_sources.txt           # Original Shakespeare sources
â”‚   â”‚   â”œâ”€â”€ legal_sources.txt                 # Legal document sources
â”‚   â”‚   â”œâ”€â”€ historical_sources.txt            # Historical speech sources
â”‚   â”‚   â””â”€â”€ biblical_sources.txt              # Religious text sources
â”‚   â”œâ”€â”€ ğŸ“ processed/
â”‚   â”‚   â”œâ”€â”€ train_data_expanded.json          # Training data (212 examples)
â”‚   â”‚   â”œâ”€â”€ val_data_expanded.json            # Validation data (45 examples)
â”‚   â”‚   â””â”€â”€ test_data_expanded.json           # Test data (47 examples)
â”‚   â””â”€â”€ validate_dataset.py                   # Dataset validation script
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset_creation.py                   # Step 4: Dataset creation
â”‚   â”œâ”€â”€ model_training.py                     # Step 5: LoRA fine-tuning
â”‚   â”œâ”€â”€ hyperparameter_optimization.py       # Step 6: Hyperparameter experiments
â”‚   â”œâ”€â”€ baseline_comparison.py               # Step 6b: Baseline evaluation
â”‚   â”œâ”€â”€ inference_pipeline.py                # Step 7: Inference system
â”‚   â”œâ”€â”€ custom_metrics.py                    # Step 8: Evaluation metrics
â”‚   â”œâ”€â”€ enhanced_training.py                 # Step 9: Training with metrics
â”‚   â””â”€â”€ modernizer.py                        # Main modernizer class
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ .gitkeep                             # Keep empty directory
â”‚   â”œâ”€â”€ historical-modernizer-final/         # Final trained model
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ tokenizer.json
â”‚   â””â”€â”€ checkpoints/                         # Training checkpoints
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ complete_pipeline.ipynb              # Full assignment notebook
â”‚   â”œâ”€â”€ data_exploration.ipynb               # Dataset analysis
â”‚   â”œâ”€â”€ model_evaluation.ipynb               # Results analysis
â”‚   â””â”€â”€ hyperparameter_analysis.ipynb       # Optimization analysis
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ setup_environment.py                 # Environment setup
â”‚   â”œâ”€â”€ download_models.py                   # Model download utility
â”‚   â”œâ”€â”€ run_training.py                      # Training script
â”‚   â””â”€â”€ evaluate_model.py                    # Evaluation script
â”œâ”€â”€ ğŸ“ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_dataset.py                      # Dataset tests
â”‚   â”œâ”€â”€ test_modernizer.py                   # Modernizer tests
â”‚   â””â”€â”€ test_metrics.py                      # Metrics tests
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ assignment_documentation.md          # Complete assignment write-up
â”‚   â”œâ”€â”€ methodology.md                       # Technical methodology
â”‚   â”œâ”€â”€ results_analysis.md                  # Results documentation
â”‚   â””â”€â”€ video_script.md                      # Video presentation script
â”œâ”€â”€ ğŸ“ results/
â”‚   â”œâ”€â”€ training_logs/                       # Training output logs
â”‚   â”œâ”€â”€ evaluation_results/                  # Evaluation outputs
â”‚   â”œâ”€â”€ visualizations/                      # Charts and graphs
â”‚   â””â”€â”€ comparison_tables/                   # Performance comparisons
â”œâ”€â”€ ğŸ“ examples/
â”‚   â”œâ”€â”€ basic_usage.py                       # Simple usage examples
â”‚   â”œâ”€â”€ batch_processing.py                  # Batch processing demo
â”‚   â””â”€â”€ interactive_demo.py                  # Interactive demonstration
â”œâ”€â”€ ğŸ“„ README.md                             # Main project documentation
â”œâ”€â”€ ğŸ“„ requirements.txt                      # Python dependencies
â”œâ”€â”€ ğŸ“„ requirements-dev.txt                  # Development dependencies
â”œâ”€â”€ ğŸ“„ setup.py                              # Package installation
â”œâ”€â”€ ğŸ“„ pyproject.toml                        # Modern Python configuration
â”œâ”€â”€ ğŸ“„ .gitignore                            # Git ignore rules
â”œâ”€â”€ ğŸ“„ .gitattributes                        # Git attributes (for LFS if needed)
â”œâ”€â”€ ğŸ“„ LICENSE                               # Project license
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md                       # Contribution guidelines
â”œâ”€â”€ ğŸ“„ CHANGELOG.md                          # Version history
â””â”€â”€ ğŸ“„ video_script.md                       # 10-minute video script

ğŸ“ˆ Performance Analysis
Hyperparameter Optimization
ConfigurationLearning RateLoRA RankVal LossPerformanceConservative1e-0589.576BaselineBalanced5e-05167.987ModerateAggressive1e-04320.633Optimal
Baseline Comparison

Pre-trained GPT-2: 40.0% accuracy (generates irrelevant content)
Fine-tuned Model: 85.2% accuracy (successful modernization)
Improvement: +45.2% performance gain

ğŸ¯ Example Transformations
Historical TextModern Text"Thou art a villain and thy words are false""You are a villain and your words are false""Wherefore dost thou weep?""Why do you weep?""Four score and seven years ago""Eighty-seven years ago""We hold these truths to be self-evident""We believe these facts are obvious"
ğŸ”¬ Evaluation Metrics
Custom Metrics Suite

BLEU Score: 79.33% (translation quality)
Semantic Similarity: 86.27% (meaning preservation)
Modernization Success: 91.96% (transformation accuracy)
Valid Output Rate: 100% (generation reliability)

Quality Control
Intelligent fallback to rule-based modernization
Multi-criteria output validation
Systematic error detection and correction

ğŸ› ï¸ Advanced Features
Inference Pipeline

Single Text Processing: Direct modernization
Batch Processing: Efficient multiple text handling
Interactive Mode: Real-time testing interface
Quality Assurance: Automatic output validation

Error Analysis & Improvements
Generation Control: Optimized parameters for focused output
Fallback Mechanisms: Rule-based reliability guarantee
Quality Assessment: Multi-dimensional evaluation framework

ğŸ“š Applications
Digital Humanities: Making historical texts accessible
Education: Interactive historical document exploration
Research: Automated analysis of archaic language patterns
Cultural Preservation: Bridging historical and contemporary language

ğŸ“ Academic Contribution
Novel Aspects
Domain-Specific Dataset: First comprehensive historical text modernization corpus
Hybrid Approach: Model + rule-based reliability system
Multi-Domain Coverage: Literature, legal, religious, and political texts
Systematic Methodology: Reproducible fine-tuning pipeline

Research Impact
Specialized NLP: Historical text processing advancement
Evaluation Framework: Custom metrics for transformation quality
Production Pipeline: End-to-end modernization system
Educational Value: Comprehensive ML methodology demonstration

ğŸ”„ Future Enhancements
Web Interface: Browser-based modernization tool
API Development: RESTful service for integration
Multi-Language Support: Extension to other historical languages
Performance Optimization: Model compression and acceleration

ğŸ“„ Citation
@software{historical_text_modernizer,
  title={Historical Text Modernizer: Fine-tuned Language Model for Archaic Text Transformation},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/historical-text-modernizer}
}
